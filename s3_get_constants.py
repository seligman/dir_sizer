#!/usr/bin/env python3

from datetime import datetime
from urllib.request import urlopen
import boto3
import json
import os

def cache_json(desc, final, save_data_filename):
    if save_data_filename is None:
        return

    temp = final["_meta"]["data downloaded"]
    final["_meta"]["data downloaded"] = ""
    updated_data = json.dumps(final, sort_keys=True)
    final["_meta"]["data downloaded"] = temp

    # Load the old version if it's available
    old_data = ""
    if os.path.isfile(save_data_filename):
        with open(save_data_filename, "rt", encoding="utf-8") as f:
            old_data = json.load(f)
            old_data["_meta"]["data downloaded"] = ""
            old_data = json.dumps(old_data, sort_keys=True)

    if old_data == updated_data:
        print("The " + desc + " in " + save_data_filename + " has not changed.")
    else:
        # Dump out the data
        with open(save_data_filename, "wt", newline="\n", encoding="utf-8") as f:
            json.dump(final, f, indent=4, sort_keys=True)
            f.write("\n")

        print("All done, created " + save_data_filename + " for " + desc)


def get_regions(save_data_filename=None):
    ssm = boto3.client('ssm', region_name="us-east-1")

    # The final data to save
    final = {"_meta": {
        "_comment": "This file is generated by " + os.path.split(__file__)[-1] + ", run that script to update it automatically.",
        "data downloaded": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ",
    )}}

    final["regions"] = {}

    paginator = ssm.get_paginator('get_parameters_by_path')
    for page in paginator.paginate(Path="/aws/service/global-infrastructure/regions"):
        for parameter in page['Parameters']:
            region = parameter['Name'].split("/")[-1]
            details = ssm.get_parameter(Name=f"/aws/service/global-infrastructure/regions/{region}/longName")
            desc = details["Parameter"]["Value"]
            final["regions"][region] = desc

    cache_json("region data", final, save_data_filename)

    return final

def get_pricing(save_data_filename=None):
    # These JSON objects are used by the AWS S3 Pricing Page at
    # https://aws.amazon.com/s3/pricing/
    urls = {
        "costs": "https://b0.p.awsstatic.com/pricing/2.0/meteredUnitMaps/s3/USD/current/s3.json",
        "deep": "https://b0.p.awsstatic.com/pricing/2.0/meteredUnitMaps/s3glacierdeeparchive/USD/current/s3glacierdeeparchive.json",
    }    

    # The final data to save
    final = {"_meta": {
        "_comment": "This file is generated by " + os.path.split(__file__)[-1] + ", run that script to update it automatically.",
        "data downloaded": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ",
    )}}

    # Load the data for each URL in turn
    for key in urls:
        urls[key] = json.load(urlopen(urls[key]))
        final["_meta"][key + " updated"] = urls[key]["manifest"]["hawkFilePublicationDate"]

    # Get all of the regions, since these JSON files from AWS drive user-visible 
    # data, all of the regions are the full English description of the region.
    regions = [x for x in urls['costs']['regions'] if len(x)]

    # Load the region names, and handle the edge cases with slightly different names
    with open("s3_regions.json") as f:
        all_regions = json.load(f)
        all_regions = {y: x for x, y in all_regions["regions"].items()}
        # This is both 'AWS GovCloud (US-East)' and 'AWS GovCloud (US)'
        all_regions["AWS GovCloud (US)"] = "us-gov-west-1"
        # The "Europe" regions are also "EU" regions
        for region_name in [x for x in all_regions if x.startswith("Europe")]:
            all_regions[region_name.replace("Europe ", "EU ")] = all_regions[region_name]

    # Raise an exception if there's a not yet known region
    for region_name in regions:
        if region_name not in all_regions:
            raise Exception(region_name + " is not a known region!")

    for region in regions:
        # Pull out the data for this region
        temp = {}
        final[all_regions[region]] = temp

        with open("s3_cost_classes.json") as f:
            s3_cost_classes = json.load(f)

        # Pull out the costs from each field in turn
        for s3_cost in s3_cost_classes:
            temp[s3_cost['desc']] = urls[s3_cost['page_source']]['regions'][region][s3_cost['page_desc']]['price']
            temp[s3_cost['desc']] = temp[s3_cost['desc']].rstrip('0')

    cache_json("pricing data", final, save_data_filename)

    return final

if __name__ == "__main__":
    get_regions(save_data_filename="s3_regions.json")
    get_pricing(save_data_filename="s3_pricing_data.json")
